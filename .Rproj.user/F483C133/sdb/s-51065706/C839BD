{
    "collab_server" : "",
    "contents" : "import os\nimport sys\n\n\nsys.path.append(\"/home/aiddata/Desktop/Github/CausalForest/scikit-learn\")\nsys.path.insert(0,\"/home/aiddata/miniconda2/lib/python2.7/site-packages/\")\n\nimport pandas as pd\nfrom sklearn import tree\nimport csv\nimport numpy as np \nimport random\n\ncsvpath = str(sys.argv[1])\nc_var = str(sys.argv[2])\no_var = str(sys.argv[3])\np_var = str(sys.argv[4])\nout_file = str(sys.argv[5])\n\n \n  # #Set 0 and 1 cases to small values to prevent division by 0 issues\n  # for(i in 1:length(matched.dta['distance'][[1]]))\n  # {\n  # \n  #   if(matched.dta['distance'][[1]][i] >= 0.99)\n  #   {\n  #     matched.dta['distance'][[1]][i] = 0.99\n  #   }\n  #   if(matched.dta['distance'][[1]][i] <= 0.01)\n  #   {\n  #     matched.dta['distance'][[1]][i] = .01\n  #   }\n  # }\n  # \n  # \n  # #Tree propensity calculations\n  # transDist <- list(rep(0,nrow(matched.dta)))\n  # for(i in 1:nrow(matched.dta))\n  # {\n  #   if(matched.dta$treatment[i] == 1)\n  #   {\n  #     #Treated\n  #     transDist[i] = matched.dta$distance[i]\n  #   }\n  #   else\n  #   {\n  #     #Untreated\n  #     transDist[i] = -1 * matched.dta$distance[i]\n  #   }\n  # }\n\n \nc_var = c_var.split(\",\")\n\nfull_dta = pd.read_csv(csvpath, header=0)\n\n\n\nX = full_dta[c_var]\nY = full_dta[o_var]\nsample_weight = full_dta[p_var]\n\n\nX = X._get_numeric_data()\n\nnames = X.columns.values \nrow_num = X.shape[0]\ncol_num = X.shape[1]\nfeature_num_ratio = 0.8\nforest_size = 100000\nres = [0]*len(Y)\nfeature_importance = [0]*col_num\n \n \ndef featureImpCal(tree, feature_importance):\n  left = tree.tree_.children_left\n  right = tree.tree_.children_right\n  features = tree.tree_.feature\n  value = tree.tree_.impurity\n  for nodeid in range(0,len(left)):\n    if features[nodeid] >= 0:\n      feature_importance[features[nodeid]] += value[nodeid] - value[left[nodeid]] - value[right[nodeid]]\n \t\t\nresultFile = open(out_file,'wb')\nwr = csv.writer(resultFile, delimiter=',')\n \t\n \nfor i in range(0,forest_size):\n  print(\"Building Tree ID:\")\n  print(i)\n  res = [0]*len(Y)\n  idx = np.random.choice(Y.index.values, int(0.8*row_num), replace=False) \n  model = tree.DecisionTreeRegressor(criterion='ct', splitter='random',max_features = feature_num_ratio,min_samples_leaf = 25, overlap_samples=0.2)\n  fitres = model.fit(np.array(X.iloc[idx], dtype=None),np.array(Y.iloc[idx], dtype=None),np.array(sample_weight.iloc[idx], dtype=None))\n  pred = model.predict(np.array(X.iloc[idx], dtype=None))\n  for j in range(0,len(idx)):\n    res[idx[j]] = pred[j]\n  wr.writerow(res)\n\nfeatureImpCal(model, feature_importance)\n \ns = feature_importance\nidx = sorted(range(len(s)), key=lambda k: s[k],reverse=True)\nnames[idx]\n\n \nimpurity_res = [0]*col_num\nfor i in range(0,col_num):\n \timpurity_res[i] = s[idx[i]]\n \nr2 = zip(names[idx],impurity_res)\nprint(r2)\n# \n# \n# \n",
    "created" : 1479313261618.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3311001804",
    "id" : "C839BD",
    "lastKnownWriteTime" : 1481036509,
    "last_content_update" : 1481036590611,
    "path" : "~/Desktop/Github/CausalForest/CF.py",
    "project_path" : "CF.py",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "python"
}